---
title: "Assignment 4"
author: '108048110'
date: "2022-11-12"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

```{r include=FALSE}
library(GGally)
library(lmtest)
require(ellipse)
```

```{r include=FALSE}
data1 = read.table('./datasets/wastes.txt', header=TRUE)
```

This is an experimental data of five laboratory measurements conducted to interpret total oxygen demand in dairy waste. Data were collected on samples kept in suspension in water in a laboratory for 220 days, and we assume that all observations are independent.

| Item | Variable | Description                  | Unit               |
|------|----------|------------------------------|--------------------|
| 1    | y        | $log(oxygen \ demand)$       | mg oxygen per unit |
| 2    | x1       | $biological\ oxygen\ demand$ | mg/liter           |
| 3    | x2       | $Total\ Kjeldahl\ nitrogen$  | mg/liter           |
| 4    | x3       | $Total\ solids$              | mg/liter           |
| 5    | x4       | $Total\ volatile\ solids$    | mg/liter           |
| 6    | x5       | $Chemical\ oxygen\ demand$   | mg/liter           |

: Total oxygen demand in dairy wastes

```{r echo=FALSE}
attach(data1)
summary(data1, cor=T)
ggpairs(data1[,-1])
```

-   $x2, x3, x5\ and\ y$ are right skewed, while $x4$ is left skewed.

-   From the correlation coefficient table we found that the estimator of $x4$, $\beta_4$ has a very high negative correlation associate with the intercept $(\beta_0)$, which might result from the range of $x4$, which compared to other variables, is relatively small.

-   As we can observe from the plot, the values of correlation coefficient between $x1\ and\ x3$, $x1\ and\ x4$, $x1\ and\ x5$, $x3\ and\ x5$, $x4\ and\ x5$ are very high, in addition, the response variable $y$ seemed to have be positively correlated with $x1, x3, x4\ and\ x5$.

### Fit a multiple regression model $y=\beta_0+\beta_1*x1+\beta_2*x2+\beta_3*x3+\beta_4*x4+\beta_5*x5+\epsilon$, using $y$ as the dependent variable and all $x_jâ€™s$ as the independent variables.

Below is the result of fitting a multiple regression model.

```{r echo=FALSE}
model1 = lm(y~x1+x2+x3+x4+x5)
smodel1 = summary(model1, cor=T)
smodel1
```

#### a. Form a 95% confidence interval for $\beta_3$ and again for $\beta_5$.

-   95% Critical value, $df_\Omega=n-p=20-6=14$

```{r echo=FALSE}
qt(0.975, 14)
```

-   General Form: $(A\hat\beta -  A\beta)^{T} (A(X^{T}X)^{-1}(A\hat\beta-A\beta) / (d * \hat\sigma^2) \le F_{d, n-p}(\alpha)$
-   95% confidence interval for $\beta_3\ and\ \beta_5$.
    -   If d=1:

$$\frac{(A\hat\beta-A\beta)^2}{(A(X^TX)^{-1}A^T)\hat\sigma^2} \le\ F_{1, n-p}(\alpha) = \frac{|A\hat\beta-A\beta|}{\sqrt(A(X^TX)^{-1}A^T)*\hat\sigma} \le t_{n-p}(\alpha/2)$$

-   In this case, $n=20, p=6, \alpha=0.05, t_{14, 0.025}=2.145$.

-   95% CI for $\beta_3$ = $A_3 = (0,0,0,1,0,0), A\hat\beta = \hat\beta_3\ , \ A_5 = (0,0,0,0,0,1), A\hat\beta = \hat\beta_5:$

    -   $\hat\beta_3 \pm t_{14}(0.025)*s.e.(\hat\beta_3) = 1.278*10^{-4}\pm2.145*7.69*10^{-5}$

    -   $\hat\beta_4 \pm t_{14}(0.025)*s.e.(\hat\beta_5) = 1.417*10^{-4} \pm 2.145 * 7.375*10^{-5}$

$confidence\ interval = estimate \pm critical\ value * s.e\ of\ estimate$

```{r echo=FALSE}
knitr::kable(confint(model1)[c(4,6),])
```

-   Both intervals contain 0, this indicates that the null hypotheses $H_0: \beta_3=0\ and\ H_0: \beta_5=0$ would not be rejected at the 5% significance level. We can see from the summary where tha p-value for $\beta_3\ and\ \beta_5$ are 11.88% and 7.54% respectively, confirming the point.

#### b. Form a confidence interval for $\beta_3+2*\beta_5$.

-   In this case, $A\hat\beta = \hat\beta_3+2*\hat\beta_5, A = (0, 0,0,1,0,2)$

```{r echo=FALSE}
A = c(0,0,0,1,0,2)
x = model.matrix(model1)
xtxi = solve(t(x)%*%x)
se = sqrt(t(A)%*%xtxi%*%A)*summary(model1)$sigma

se
```

-   $\beta_3+2*\beta_5 = (\hat\beta_3+2*\hat\beta_5) \pm 2.145 * 1.642*10^{-4}$

#### c. Show a 95% C.R. graph for $\beta_3\ and\ \beta_5$. Plot the origin. State the hypothesis test and its outcome.

```{r echo=FALSE}
plot(ellipse(model1, c(4, 6)), type='l')
points(coef(model1)[4], coef(model1)[6], pch=19)
abline(v = confint(model1)[4,], lty=2)
abline(h = confint(model1)[6,], lty=2)
points(0, 0, pch=19, col='red')
```

```{r echo=FALSE}
summary(model1, cor=T)$cor[4, 6]
```

-   The seemly parallelism of the major axis of the ellipse to the y-axis and that of the semi-minor axis to the x-axis suggests that the two estimators, $\beta_3, \beta_5$ are uncorrelated.

-   We can deduced from the plot that the joint hypothesis $H_0: \beta_3=\beta_5=0$ is not reject because the origin lies right inside the ellipse

-   Both of the hypotheses $H_0: \beta_3=0\ and\ H_0: \beta_5=0$ are not rejected either because 0 does lie within the vertical and horizontal dashed lines which represents as the C.I. of $x3\ and\ x5$ respectively.

-   We can further conduct some experiments testing on the significance of $\beta_3$ where x5 is included in the model and the significance of $\beta_5$ where $x3$ is included in the model to ensure that both variables $x3, x5$ are not significant when the true model is assumed to be $y=\beta_0+\beta_1*x1+\beta_2*x2+\beta_3*x3+\beta_4*x4+\beta_5*x5+\epsilon$.

```{r echo=FALSE}
model2 = lm(y~x1+x2+x4+x5)
model3 = lm(y~x1+x2+x3+x4)
model4 = lm(y~x1+x2+x4)

anova(model2, model4)
anova(model3, model4)
```

-   In conclusion, as we can observe from the above tests and visualization, all of our hypotheses are not rejected, which indicate that neither of the estimators, $\beta_3, \beta_5$ are significant enough to be indispensable for response variable $y$ under the 95% confidence level.

#### d. If a 95% joint confidence region was computed for $\beta_1, \beta_2, \beta_3, \beta_4, \beta_5$, would the origin lie inside or outside the region? Explain.

```{r echo=FALSE}
knitr::kable(confint(model1))
knitr::kable(confint(model1)[,1]<0 & confint(model1)[,2]>0)
```

-   As we can observe from the table, if measured individually, none of the predictors are significant; however, combining univariate tests to test on joint relationship may come up with a biased result, thus we need to conduct a test to compute the boundaries of the joint confidence region.

```{r echo=FALSE}

#= cbind(rep(0, 6), diag(5))

X = as.matrix(cbind(rep(1, 20), data1[,c(-1, -7)]))

e <- eigen(t(X)%*%X)
A = t(e$vectors[,-1])

betahat = matrix(model1$coefficients, nrow=6, ncol=1)
beta = matrix(0L, nrow=6, ncol=1)
critical_value = 2*(smodel1$sigma^2)*qf(0.05, 5, 14, lower.tail = TRUE)
```

**Joint Effect**

```{r echo=FALSE}
t(A%*%betahat-A%*%beta)%*%solve(A%*%(solve(t(X)%*%X))%*%t(A))%*%(A%*%betahat-A%*%beta)
```

**Critical Value**

```{r echo=FALSE}
critical_value
```

-   After testing the joint effect of $\beta_1, \beta_2, \beta_3, \beta_4, \beta_5$, we found the calculated result is significantly larger than the corresponding critical value $2*\hat\sigma^2* F_{5, 14} = 0.02957$, which implies that the origin $(0,0,0,0,0)$ is lying outside the region.

-   Thus, we can reject the null hypothesis that $H_0: \beta_1=\beta_2=\beta_3=\beta_4=\beta_5=0$.

#### e. Suppose non-volatile solids have no linear effect on the response. State a hypothesis that reflects this suspicion, and test it using a C.I. in your answer to one of the above questions. Explain why the chosen confidence interval can be used to do this work.

-   Arrange our previous model: $y=\beta_0+\beta_1*x1+\beta_2*x2+\beta_3*x3+\beta_4*x4+\beta_5*x5+\epsilon$

-   Total solids - volatile solids = non-volatile solids: $y=\beta_0+\beta_1*x1+\beta_2*x2+\beta_3*(x3-x4)+\beta_4*x4+\beta_5*x5+\epsilon$

-   $y=\beta_0+\beta_1*x1+\beta_2*x2+\beta_3*x3+(\beta_4-\beta_3)*x4+\beta_5*x5+\epsilon$

-   $y=\beta_0+\beta_1*x1+\beta_2*x2+\beta_3*x3+\beta_4^{'}*x4+\beta_5*x5+\epsilon,\  where\ \beta_4^{'} = \beta_4-\beta_3$

-   As we can observe from the model equation, originally, $x4 \subset x3$, where $\beta_3$ is an estimator for both volatile and non-volatile solids while $\beta_4$ is an estimator for volatile solids, and therefore $\beta_4$ explains both regular parts from $x3\ and\ x4$; yet with the arrangement, now $\beta_3$ becomes the estimator for non-volatile solids which is uncorrelated to predictor $x4$.

-   Hence, we hypothesized that $H_0: \beta_3=0$, Since the arrangement is only a linear transformation performed on variable $x3$ which does not change the value of $\beta_3$, I would apply the C.I. value calculated from question **a** to test this hypothesis.

    -   Set $\beta_3=0$

        ```{r echo=FALSE}
        beta_3 = 0
        ```

    -   Introduce C.I. for $\beta_3$ from model1

        ```{r echo=FALSE}
        CI3 = confint(model1)[4,]
        CI3
        ```

    -   Whether we accept the null hypothesis

        ```{r echo=FALSE}
        (beta_3 > CI3[1])&&(beta_3 < CI3[2])
        ```

<!-- -->

-   Since 0 lies in the C.I. of $\beta_3$, we do not reject the null hypothesis, that is, the non-volatile solids have no linear effect on the response.

```{r echo=FALSE}
detach(data1)
```

## Problem 2

```{r echo=FALSE}
data2 = read.table('http://www.stat.nthu.edu.tw/~swcheng/Teaching/stat5410/data/houseprices.txt', header=TRUE)
```

This data are a random sample of home sales from Spring 1993 in Albuquerque.

+------------+------------+-------------------------------------+-------------------+
| Item       | Variable   | Description                         | Unit              |
+============+============+=====================================+===================+
| 1          | Price      | Selling price                       | $\$100$           |
+------------+------------+-------------------------------------+-------------------+
| 2          | SQFT       | living space                        | $feet^2$          |
+------------+------------+-------------------------------------+-------------------+
| 3          | Age        | Age of home                         | $year$            |
+------------+------------+-------------------------------------+-------------------+
| 4          | Features   | Number out of 11 features           | (dish washer,     |
|            |            |                                     |                   |
|            |            |                                     | refrigerator,     |
|            |            |                                     |                   |
|            |            |                                     | microwave,        |
|            |            |                                     |                   |
|            |            |                                     | disposer,         |
|            |            |                                     |                   |
|            |            |                                     | washer, intercom, |
|            |            |                                     |                   |
|            |            |                                     | skylight(s),      |
|            |            |                                     |                   |
|            |            |                                     | compactor,        |
|            |            |                                     |                   |
|            |            |                                     | dryer,            |
|            |            |                                     |                   |
|            |            |                                     | handicap fit,     |
|            |            |                                     |                   |
|            |            |                                     | cable TV access)  |
+------------+------------+-------------------------------------+-------------------+
| 5          | NE         | Located in northeast sector of city | $1\ or\ 0$        |
+------------+------------+-------------------------------------+-------------------+
| 6          | Corner     | Corner location                     | $1\ or\ 0$        |
+------------+------------+-------------------------------------+-------------------+
| 7          | Taxes      | Annual taxes                        | $\$$              |
+------------+------------+-------------------------------------+-------------------+

: Home sales data

#### a. There are a large number of missing values in the $Age$ variable. We could either exclude $Age$ from our models for the selling price or we could keep $Age$ and exclude the cases that have missing values for $Age$. Which choice is better for this data? Explain your reasoning.

-   Check how many missing values are in the data.

```{r echo=FALSE}
summary(data2)
```

-   As we can observe from the result of the summary, there are 49 na values in variable $Age$ and 10 in variable $Tax$.

-   Furthermore, although $Features$ variable description indicated there are 11 different features in the data, yet as we can see from the table, there are only 8 features at most including in our sample data.

```{r echo=FALSE}
nrow(data2)
```

-   However, there are only 117 samples in this data, removing all the missing values in $Age$ would indicate dropping nearly half the amount of the observations, which is irrational to give up so much observations; therefore, choosing to exclude variable $Age$ from the dataset is a more economical and reasonable option.

```{r echo=FALSE}
knitr::kable(head(data2[,-3], 6))
data2 = data2[,-3]
data2 = data2[complete.cases(data2),]
knitr::kable(summary(data2))
attach(data2)
```

-   New data after excluding $Age$ from the dataset and removing na values from variable $Tax$.

```{r echo=FALSE}
ggpairs(data2)
```

-   House $Price$ seems to be positively correlated with $SQFT, Tax$ which maps to our intuition. While the relationship between $Price$ and other variables are unclear.

#### b. Fit a model with $Price$ as the response and $SQFT, Features, NE, Corner,Tax$ as predictors. Form 95% and 99% C.I. for their coefficients. Explain how the p-value for the parameter for $Corner$ relates to whether zero falls in the two corresponding C.I..

$Price = \beta_0+\beta_1*SQFT+\beta_2*Features+\beta_3*NE+\beta_4*Corner+\beta_5*Tax+\epsilon$

```{r echo=FALSE}
model1 = lm(Price~SQFT+factor(Features)+factor(NE)+factor(Corner)+Tax)
model2 = lm(Price~SQFT+Features+NE+Corner+Tax)
model3 = lm(Price~SQFT+Features+factor(NE)+factor(Corner)+Tax)
smodel1 = summary(model1, cor=T)
smodel2 = summary(model2, cor=T)
smodel3 = summary(model3, cor=T)

smodel1
#smodel2
#smodel3
```

-   **Note.** Features, Corner and NE are qualitative data, the numeric value are categorical.

```{r echo=FALSE}
knitr::kable(confint(model1, level=0.95))
knitr::kable(confint(model1, level=0.99))
```

-   p-value for $Corner$ is not significant whether the significance level is 95% or 99%. Since zero falls in both $\alpha=5\%, \alpha=1\%$ confidence intervals.

#### c. Predict the $Price$ of a specific house with $SQFT=2500, Features=5, NE=1, Corner=1, Tax=120$. Give an appropriate 95% C.I..

$$Price = 154.548+0.253*SQFT-34.356*F_1-44.180*F_2-31.409*F_3-44.753*F_4+8.071*F_5+35.905*F_6+98.549*F_7+118.434*F_8-4.563*NE+-83.752*Corner+0.690*Tax+\epsilon$$

-   **Note.** $If\ Featurs=x, F_x = 1, x\in[1,8]$

```{r echo=FALSE}
predict(model1, data.frame(SQFT=2500, Features=5, NE=1, Corner=1, Tax=1200), interval="confidence")
```

-   This is an interpolation prediction computing for the C.I. of mean response.

#### d. Suppose you are only told that $SQFT=2500$. Predict the $Price$ and 95% C.I..

##### When all variables have low correlation:

```{r}
model2 = lm(Price~., data2)
x = data.frame(SQFT=2500, Features = median(Features),NE=which.max(table(NE)),Corner = which.max(table(Corner)),Tax = mean(Tax))
                 
predict(model2, newdata=x,interval="predict")
```

-   Conditional Probability

##### Other method:

```{r}
fit2.3 = lm(Price ~ SQFT, data2) 
predict(fit2.3, newdata=data.frame(SQFT=2500),interval="predict")
```
